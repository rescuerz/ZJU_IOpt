@book{nesterov2013introductory,
	title={Introductory lectures on convex optimization: A basic course},
	author={Nesterov, Yurii},
	volume={87},
	year={2013},
	publisher={Springer Science \& Business Media}
}

@inproceedings{defazio2016simple,
	title={A simple practical accelerated method for finite sums},
	author={Defazio, Aaron},
	booktitle={Advances in Neural Information Processing Systems},
	pages={676--684},
	year={2016}
}

@article{schmidt2017minimizing,
	title={Minimizing finite sums with the stochastic average gradient},
	author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
	journal={Mathematical Programming},
	volume={162},
	number={1-2},
	pages={83--112},
	year={2017},
	publisher={Springer}
}

@article{necoara2017random,
	title={Random block coordinate descent methods for linearly constrained optimization over networks},
	author={Necoara, Ion and Nesterov, Yurii and Glineur, Fran{\c{c}}ois},
	journal={Journal of Optimization Theory and Applications},
	volume={173},
	number={1},
	pages={227--254},
	year={2017},
	publisher={Springer}
}

@phdthesis{johansson2008distributed,
	title={On distributed optimization in networked systems},
	author={Johansson, Bj{\"o}rn},
	year={2008},
	school={KTH}
}

@article{bazerque2010distributed,
	title={Distributed spectrum sensing for cognitive radio networks by exploiting sparsity},
	author={Bazerque, Juan Andr{\'e}s and Giannakis, Georgios B},
	journal={IEEE Transactions on Signal Processing},
	volume={58},
	number={3},
	pages={1847--1862},
	year={2010},
	publisher={IEEE}
}

@article{forero2010consensus,
	title={Consensus-based distributed support vector machines},
	author={Forero, Pedro A and Cano, Alfonso and Giannakis, Georgios B},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={May},
	pages={1663--1707},
	year={2010}
}



@article{nedic2009distributed,
	title={Distributed subgradient methods for multi-agent optimization},
	author={Nedic, Angelia and Ozdaglar, Asuman},
	journal={IEEE Transactions on Automatic Control},
	volume={54},
	number={1},
	pages={48--61},
	year={2009},
	publisher={IEEE}
}

@article{shi2015extra,
	title={Extra: An exact first-order algorithm for decentralized consensus optimization},
	author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={2},
	pages={944--966},
	year={2015},
	publisher={SIAM}
}

@article{davis2015convergence,
	title={Convergence rate analysis of primal-dual splitting schemes},
	author={Davis, Damek},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={3},
	pages={1912--1943},
	year={2015},
	publisher={SIAM}
}

@article{mokhtari2016dsa,
	title={{DSA}: {Decentralized} double stochastic averaging gradient algorithm},
	author={Mokhtari, Aryan and Ribeiro, Alejandro},
	journal={Journal of Machine Learning Research},
	volume={17},
	number={61},
	pages={1--35},
	year={2016}
}

@article{yuan2016convergence,
	title={On the convergence of decentralized gradient descent},
	author={Yuan, Kun and Ling, Qing and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={26},
	number={3},
	pages={1835--1854},
	year={2016},
	publisher={SIAM}
}

@article{shi2014linear,
	title={On the Linear Convergence of the ADMM in Decentralized Consensus Optimization.},
	author={Shi, Wei and Ling, Qing and Yuan, Kun and Wu, Gang and Yin, Wotao},
	journal={IEEE Trans. Signal Processing},
	volume={62},
	number={7},
	pages={1750--1761},
	year={2014}
}

@book{bullo2009distributed,
  title={Distributed control of robotic networks: a mathematical approach to motion coordination algorithms},
  author={Bullo, Francesco and Cortes, Jorge and Martinez, Sonia},
  year={2009},
  publisher={Princeton University Press}
}

@article{ribeiro2010ergodic,
  title={Ergodic stochastic optimization algorithms for wireless communication and networking},
  author={Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing},
  volume={58},
  number={12},
  pages={6369--6386},
  year={2010},
  publisher={IEEE}
}

@article{mokhtari2017network,
  title={Network {Newton} distributed optimization methods},
  author={Mokhtari, Aryan and Ling, Qing and Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={1},
  pages={146--161},
  year={2017},
  publisher={IEEE}
}

@article{eisen2017decentralized,
  title={Decentralized quasi-{Newton} methods},
  author={Eisen, Mark and Mokhtari, Aryan and Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing},
  volume={65},
  number={10},
  pages={2613--2628},
  year={2017},
  publisher={IEEE}
}

@article{ling2015dlm,
	title={{DLM}: {Decentralized} linearized alternating direction method of multipliers},
	author={Ling, Qing and Shi, Wei and Wu, Gang and Ribeiro, Alejandro},
	journal={IEEE Transactions on Signal Processing},
	volume={63},
	number={15},
	pages={4051--4064},
	year={2015},
	publisher={IEEE}
}

@article{mokhtari2015decentralized,
  title={{DQM}: {Decentralized} Quadratically Approximated Alternating Direction Method of Multipliers},
  author={Mokhtari, Aryan and Shi, Wei and Ling, Qing and Ribeiro, Alejandro},
  journal={IEEE Transactions on Signal Processing},
  volume={64},
  number={19},
  pages={5158--5173},
  year= {2016},
}

@article{duchi2012dual,
	title={Dual averaging for distributed optimization: Convergence analysis and network scaling},
	author={Duchi, John C and Agarwal, Alekh and Wainwright, Martin J},
	journal={IEEE Transactions on Automatic control},
	volume={57},
	number={3},
	pages={592--606},
	year={2012},
	publisher={IEEE}
}

@inproceedings{scaman2017optimal,
	title={Optimal Algorithms for Smooth and Strongly Convex Distributed Optimization in Networks},
	author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
	booktitle={International Conference on Machine Learning},
	pages={3027--3036},
	year={2017}
}

@inproceedings{ying2016stochastic,
	title={Stochastic online AUC maximization},
	author={Ying, Yiming and Wen, Longyin and Lyu, Siwei},
	booktitle={Advances in neural information processing systems},
	pages={451--459},
	year={2016}
}

@book{bauschke2017convex,
	title={Convex analysis and monotone operator theory in Hilbert spaces},
	author={Bauschke, Heinz H and Combettes, Patrick L and others},
	volume={2011},
	year={2017},
	publisher={Springer}
}

@article{rockafellar1970monotone,
	title={Monotone operators associated with saddle-functions and minimax problems},
	author={Rockafellar, R Tyrrell},
	journal={Nonlinear functional analysis},
	volume={18},
	number={part 1},
	pages={397--407},
	year={1970},
	publisher={Proceedings of Symposia in Pure Mathematics, American Mathematical Society}
}

@article{rockafellar1970maximal,
	title={On the maximal monotonicity of subdifferential mappings},
	author={Rockafellar, RT and others},
	journal={Pacific J. Math},
	volume={33},
	number={1},
	pages={209--216},
	year={1970}
}

@inproceedings{wu2016decentralized,
	title={Decentralized consensus optimization with asynchrony and delays},
	author={Wu, Tianyu and Yuan, Kun and Ling, Qing and Yin, Wotao and Sayed, Ali H},
	booktitle={Signals, Systems and Computers, 2016 50th Asilomar Conference on},
	pages={992--996},
	year={2016},
	organization={IEEE}
}

@article{shi2015proximal,
	title={A proximal gradient algorithm for decentralized composite optimization},
	author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
	journal={IEEE Transactions on Signal Processing},
	volume={63},
	number={22},
	pages={6013--6023},
	year={2015},
	publisher={IEEE}
}

@inproceedings{defazio2014saga,
	title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
	author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	booktitle={Advances in neural information processing systems},
	pages={1646--1654},
	year={2014}
}

@article{xiao2004fast,
	title={Fast linear iterations for distributed averaging},
	author={Xiao, Lin and Boyd, Stephen},
	journal={Systems \& Control Letters},
	volume={53},
	number={1},
	pages={65--78},
	year={2004},
	publisher={Elsevier}
}

@inproceedings{colin2016gossip,
	title={Gossip dual averaging for decentralized optimization of pairwise functions},
	author={Colin, Igor and Bellet, Aur{\'e}lien and Salmon, Joseph and Cl{\'e}men{\c{c}}on, St{\'e}phan},
	booktitle={International Conference on Machine Learning},
	pages={1388--1396},
	year={2016}
}

@inproceedings{gao2013one,
	title={One-pass AUC optimization},
	author={Gao, Wei and Jin, Rong and Zhu, Shenghuo and Zhou, Zhi-Hua},
	booktitle={International Conference on Machine Learning},
	pages={906--914},
	year={2013}
}

@article{hanley1982meaning,
	title={The meaning and use of the area under a receiver operating characteristic (ROC) curve.},
	author={Hanley, James A and McNeil, Barbara J},
	journal={Radiology},
	volume={143},
	number={1},
	pages={29--36},
	year={1982}
}

@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in neural information processing systems},
	pages={315--323},
	year={2013}
}

@InProceedings{pmlr-v54-leblond17a,
	title = 	 {{ASAGA: Asynchronous Parallel SAGA}},
	author = 	 {RÃ©mi Leblond and Fabian Pedregosa and Simon Lacoste-Julien},
	booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {46--54},
	year = 	 {2017},
	editor = 	 {Aarti Singh and Jerry Zhu},
	volume = 	 {54},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {20--22 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v54/leblond17a/leblond17a.pdf},
	url = 	 {http://proceedings.mlr.press/v54/leblond17a.html},
	abstract = 	 {We describe ASAGA, an asynchronous parallel version of the incremental gradient algorithm SAGA that enjoys fast linear convergence rates. Through a novel perspective, we revisit and clarify a subtle but important technical issue present in a large fraction of the recent convergence rate proofs for asynchronous parallel optimization algorithms, and propose a simplification of the recently introduced âperturbed iterateâ framework that resolves it. We thereby prove that ASAGA can obtain a theoretical linear speedup on multi-core systems even without sparsity assumptions. We present results of an implementation on a 40-core architecture illustrating the practical speedup as well as the hardware overhead.}
}

@article{tsitsiklis1986distributed,
	title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
	author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
	journal={IEEE transactions on automatic control},
	volume={31},
	number={9},
	pages={803--812},
	year={1986},
	publisher={IEEE}
}

@book{bertsekas1989parallel,
	title={Parallel and distributed computation: numerical methods},
	author={Bertsekas, Dimitri P and Tsitsiklis, John N},
	volume={23},
	year={1989},
	publisher={Prentice hall Englewood Cliffs, NJ}
}

@article{boyd2006randomized,
	title={Randomized gossip algorithms},
	author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
	journal={IEEE transactions on information theory},
	volume={52},
	number={6},
	pages={2508--2530},
	year={2006},
	publisher={IEEE}
}
