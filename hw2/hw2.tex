\documentclass{article}
\usepackage{hwopt}
\newcommand{\RBB}{\mathbb{R}}
\newcommand{\xB}{\mathbf{x}}
\newcommand{\yB}{\mathbf{y}}
%%%%%%%%%%%%%%%%%
%     Title     %
%%%%%%%%%%%%%%%%%
\title{Coursework (2) for \emph{Introductory Lectures on Optimization}}
\author{Zhou Nan \\ 3220102535}
\date{\today}

\begin{document}
\maketitle

\begin{excercise}\label{e0}
For the function $f(x): \mathbb{R}^n \rightarrow \mathbb{R}^m$, please write down the zeroth-order Taylor expansion with an integral remainder term.
\end{excercise}

\begin{SOLUTION}{e0}
The zeroth-order Taylor expansion with an integral remainder term is
\[
f(x) = f(x_0) + \int_0^1 \nabla f(x_0 + \tau(x - x_0)) \cdot (x- x_0) d\tau
\]
\end{SOLUTION}

\begin{excercise}\label{e1}
Please write down the definition of the $p$-norm for a $n$-dimensional real vector.
\end{excercise}

\begin{SOLUTION}{e1}
Suppose a vector $\xB$ is a $n$-dimensional real vector. $\xB = (x_1, x_2, \cdots, x_n)$
\[
	\|\xB\|_p = \left(\sum_{i=1}^n |x_i|^p\right)^{1/p}.
\]
\end{SOLUTION}

\begin{excercise}\label{e2}
Please write down the definition of the matrix norms induced by vector $p$-norms.
\end{excercise}

\begin{SOLUTION}{e2}
For a given matrix $A \in \RBB^{m \times n}$, and a vector p-norm $\|\xB\|_p$
\[
	\|A\|_p = \sup_{\xB \neq 0} \frac{\|A\xB\|_p}{\|\xB\|_p}.
\]
\end{SOLUTION}

\newpage
\begin{excercise}\label{e3}
	Let $A$ be an $n \times n$ symmetric matrix. Proof that $A$ is positive semidefinite if and only if all eigenvalues of $A$ are nonnegative. Moreover, $A$ is positive definite if and only if all eigenvalues of $A$ are positive.
\end{excercise}

\begin{PROOF}{e3}

	According to the spectural theorem, 
	\[
		A = Q \Lambda Q^T
	\]
	\begin{itemize}
	 	\item Matrix \( Q \):
		
		\( Q \) is an orthogonal matrix (or unitary matrix in the complex case), meaning that its columns are orthonormal vectors (i.e., \( Q^T Q = I \), where \( I \) is the identity matrix). The columns of \( Q \) are the eigenvectors of the matrix \( A \).
	 
	 	\item Matrix \( \Lambda \):
		
		\( \Lambda \) is a diagonal matrix that contains the eigenvalues of \( A \) on its diagonal. The eigenvalues correspond to the scaling factors associated with their respective eigenvectors.
	 
	\end{itemize}

	\begin{enumerate}
		\item If A is positive semidefinite, then all eigenvalues of A are nonnegative.
		
		Since A is positive semidefinite, then for any n-dimensional vector $\xB$, we have
		\[
		\begin{aligned}
			\xB^T A \xB \geq 0\\ 
			\xB^T Q \Lambda Q^T \xB \geq 0
		\end{aligned}
		\]
		Suppose there is a eigenvalue $\lambda_i$ is negative, then we construct a 
		n-dimensional vector $\yB$ such that $\yB = (0, 0, \cdots, 0, 1, 0, \cdots, 0)$, $y_i$ = 1.

		Let $\xB = Q \yB$, then
		\[
			\xB^T A \xB = (Q\yB)^T A (Q\yB) = (Q\yB)^T Q \Lambda Q^T (Q\yB) = \yB^T Q^T Q \Lambda Q^T Q \yB =  \yB^T \Lambda \yB = \lambda_i < 0	
		\]
		which is a contradiction.
		\item If all eigenvalues of A are nonnegative, then A is positive semidefinite.
		
		For any n-dimensional vector $\xB$, we have
		\[
			\xB^T A \xB = \xB^T Q \Lambda Q^T \xB = (Q\xB)^T \Lambda (Q\xB) = \sum_{i=1}^n \lambda_i (Q^T \xB)_i^2 \geq 0
		\]
		Therefore, A is positive semidefinite.
		\item If A is positive definite, then all eigenvalues of A are positive.
		
		Since A is positive definite, then for any n-dimensional vector $\xB$, we have
		\[
			\xB^T A \xB > 0
		\]
		Suppose there is a eigenvalue $\lambda_i$ is not positive, then we construct a 
		n-dimensional vector $\yB$ such that $\yB = (0, 0, \cdots, 0, 1, 0, \cdots, 0)$, $y_i$ = 1.

		Let $\xB = Q \yB$, then
		\[
			\xB^T A \xB = (Q\yB)^T A (Q\yB) = \yB^T \Lambda \yB = \lambda_i \leq 0	
		\]
		which is a contradiction.
		
		\item If all eigenvalues of A are positive, then A is positive definite.
		
		For any n-dimensional vector $\xB$, we have
		\[
		\xB^T A \xB = \xB^T Q \Lambda Q^T \xB = (Q\xB)^T \Lambda (Q\xB) = \sum_{i=1}^n \lambda_i (Q^T \xB)_i^2 > 0
		\]
		Therefore, A is positive definite.
	\end{enumerate}
\end{PROOF}
\newpage
\begin{excercise}\label{e4}
Suppose that $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and upper bounded. Show that $f$ must
be a constant function.
\end{excercise}

\begin{PROOF}{e4}
	Suppose that $f$ is not a constant function. Then we have: $\exists \xB_1 \in \RBB^n, \xB_2 \in \RBB^n$ such that $f(\xB_1) > f(\xB_2)$ 

	Since $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex, we have:
	\[
	\begin{aligned}
		f(\xB_2) \geq f(\xB_1) + \langle \nabla f(\xB_1), \xB_2 - \xB_1\rangle \\
	\end{aligned}
	\]
	Because $f(\xB_1) > f(\xB_2)$, then $\langle \nabla f(\xB_1), \xB_2 - \xB_1\rangle < 0$

	Since $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is upper bounded, we have:
	\[
		\exists M, \forall \xB \in \RBB^n, f(\xB) \leq M
	\]
	
	Let G be $\dfrac{M+1-f(\xB_1)}{\langle \nabla f(\xB_1), \xB_2 - \xB_1\rangle }$, let $\yB = (1- G)\xB_1 + G \xB_2$, then
	\[
		\begin{aligned}
			f(\yB) &\geq f(\xB_1) + \langle \nabla f(\xB_1), \yB - \xB_1\rangle \\
			&= f(\xB_1) + \langle \nabla f(\xB_1), (1- G)\xB_1 + G \xB_2 - \xB_1\rangle \\
			&= f(\xB_1) + \langle \nabla f(\xB_1), G(\xB_2 - \xB_1) \rangle\\
			&= f(\xB_1) + G \langle \nabla f(\xB_1), \xB_2 - \xB_1\rangle\\
			&= f(\xB_1) + M + 1 - f(\xB_1) \\
			&= M + 1
		\end{aligned}
	\]
	Therefore there exists $\yB \in \RBB^n$ such that $f(\yB) \geq M + 1$, which is a contradiction.

	Therefore $f$ is a constant function.
\end{PROOF}

\end{document}


